# -*- coding: utf-8 -*-
"""
forgetting_agent.py - é—å¿˜ä»£ç†
ä½œä¸ºä¸€ä¸ªåå°ä»»åŠ¡ï¼Œå®šæœŸæ¸…ç†é™ˆæ—§çš„ã€ä¸é‡è¦çš„è®°å¿†ï¼Œæ¨¡æ‹Ÿäººç±»çš„é—å¿˜æ›²çº¿ã€‚
"""

import asyncio
import json
from typing import Dict, Any, Optional

from astrbot.api import logger
from astrbot.api.star import Context
from ...storage.faiss_manager import FaissManager
from ..utils import get_now_datetime, safe_parse_metadata, validate_timestamp


class ForgettingAgent:
    """
    é—å¿˜ä»£ç†ï¼šä½œä¸ºä¸€ä¸ªåå°ä»»åŠ¡ï¼Œå®šæœŸæ¸…ç†é™ˆæ—§çš„ã€ä¸é‡è¦çš„è®°å¿†ï¼Œæ¨¡æ‹Ÿäººç±»çš„é—å¿˜æ›²çº¿ã€‚
    """

    def __init__(
        self, context: Context, config: Dict[str, Any], faiss_manager: FaissManager
    ):
        """
        åˆå§‹åŒ–é—å¿˜ä»£ç†ã€‚

        Args:
            context (Context): AstrBot çš„ä¸Šä¸‹æ–‡å¯¹è±¡ã€‚
            config (Dict[str, Any]): æ’ä»¶é…ç½®ä¸­ 'forgetting_agent' éƒ¨åˆ†çš„å­—å…¸ã€‚
            faiss_manager (FaissManager): æ•°æ®åº“ç®¡ç†å™¨å®ä¾‹ã€‚
        """
        self.context = context
        self.config = config
        self.faiss_manager = faiss_manager
        self._task: Optional[asyncio.Task] = None
        self._manual_task: Optional[asyncio.Task] = None
        self._operation_lock = asyncio.Lock()

        # è®°å½•é…ç½®ä¿¡æ¯
        enabled = config.get("enabled", True)
        retention_days = config.get("retention_days", 90)
        check_interval_hours = config.get("check_interval_hours", 24)
        decay_rate = config.get("importance_decay_rate", 0.005)
        importance_threshold = config.get("importance_threshold", 0.1)

        logger.info("ForgettingAgent åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"  å¯ç”¨çŠ¶æ€: {'æ˜¯' if enabled else 'å¦'}")
        logger.info(f"  ä¿ç•™å¤©æ•°: {retention_days} å¤©")
        logger.info(f"  æ£€æŸ¥é—´éš”: {check_interval_hours} å°æ—¶")
        logger.info(f"  è¡°å‡ç‡: {decay_rate}/å¤©")
        logger.info(f"  é‡è¦æ€§é˜ˆå€¼: {importance_threshold}")

    async def start(self):
        """å¯åŠ¨åå°é—å¿˜ä»»åŠ¡ã€‚"""
        if not self.config.get("enabled", True):
            logger.info("é—å¿˜ä»£ç†æœªå¯ç”¨ï¼Œä¸å¯åŠ¨åå°ä»»åŠ¡ã€‚")
            return

        if self._task is None or self._task.done():
            self._task = asyncio.create_task(self._run_periodically())
            logger.info("é—å¿˜ä»£ç†åå°ä»»åŠ¡å·²å¯åŠ¨ã€‚")

    async def stop(self):
        """åœæ­¢åå°é—å¿˜ä»»åŠ¡ã€‚"""
        if self._task and not self._task.done():
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                logger.info("é—å¿˜ä»£ç†åå°ä»»åŠ¡å·²æˆåŠŸå–æ¶ˆã€‚")
        self._task = None

    async def trigger_manual_run(self) -> Dict[str, Any]:
        """æ‰‹åŠ¨è§¦å‘é—å¿˜ä»»åŠ¡çš„å…¬å…±æ¥å£,ä½¿ç”¨é”é˜²æ­¢ç«æ€æ¡ä»¶ã€‚

        Returns:
            Dict[str, Any]: åŒ…å« 'success' å’Œ 'message' çš„å“åº”å­—å…¸
        """
        async with self._operation_lock:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„æ‰‹åŠ¨ä»»åŠ¡
            if self._manual_task and not self._manual_task.done():
                return {
                    "success": False,
                    "message": "é—å¿˜ä»»åŠ¡æ­£åœ¨è¿è¡Œä¸­,è¯·ç¨åå†è¯•"
                }

            try:
                logger.info("æ‰‹åŠ¨è§¦å‘é—å¿˜ä»£ç†ä»»åŠ¡...")
                await self._prune_memories()
                return {
                    "success": True,
                    "message": "é—å¿˜ä»£ç†ä»»åŠ¡æ‰§è¡Œå®Œæ¯•"
                }
            except Exception as e:
                logger.error(f"æ‰‹åŠ¨è§¦å‘é—å¿˜ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
                return {
                    "success": False,
                    "message": f"é—å¿˜ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}"
                }

    async def _run_periodically(self):
        """åå°ä»»åŠ¡çš„å¾ªç¯ä½“ã€‚"""
        interval_hours = self.config.get("check_interval_hours", 24)
        interval_seconds = interval_hours * 3600
        logger.info(f"ğŸ• é—å¿˜ä»£ç†å®šæœŸä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯ {interval_hours} å°æ—¶è¿è¡Œä¸€æ¬¡")

        while True:
            try:
                logger.debug(f"â° ç­‰å¾… {interval_hours} å°æ—¶åæ‰§è¡Œä¸‹ä¸€æ¬¡æ¸…ç†...")
                await asyncio.sleep(interval_seconds)

                logger.info("ğŸ§¹ å¼€å§‹æ‰§è¡Œè®°å¿†æ¸…ç†ä»»åŠ¡...")
                await self._prune_memories()
                logger.info("âœ… è®°å¿†æ¸…ç†ä»»åŠ¡æ‰§è¡Œå®Œæ¯•")

            except asyncio.CancelledError:
                logger.info("ğŸ›‘ é—å¿˜ä»£ç†ä»»åŠ¡å·²è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(
                    f"âŒ é—å¿˜ä»£ç†åå°ä»»åŠ¡å‘ç”Ÿé”™è¯¯: {type(e).__name__}: {e}",
                    exc_info=True
                )
                # å³ä½¿å‡ºé”™ï¼Œä¹Ÿç­‰å¾…ä¸‹ä¸€ä¸ªå‘¨æœŸï¼Œé¿å…å¿«é€Ÿå¤±è´¥åˆ·å±
                logger.warning(f"â³ ç­‰å¾… 60 ç§’åé‡è¯•...")
                await asyncio.sleep(60)

    async def _prune_memories(self):
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„è®°å¿†è¡°å‡å’Œä¿®å‰ªï¼Œä½¿ç”¨åˆ†é¡µå¤„ç†é¿å…å†…å­˜è¿‡è½½ã€‚"""
        try:
            # è·å–è®°å¿†æ€»æ•°
            total_memories = await self.faiss_manager.count_total_memories()
            if total_memories == 0:
                logger.info("ğŸ“­ æ•°æ®åº“ä¸­æ²¡æœ‰è®°å¿†ï¼Œæ— éœ€æ¸…ç†")
                return

            retention_days = self.config.get("retention_days", 90)
            decay_rate = self.config.get("importance_decay_rate", 0.005)
            importance_threshold = self.config.get("importance_threshold", 0.1)
            current_time = get_now_datetime(self.context).timestamp()

            # åˆ†é¡µå¤„ç†é…ç½®
            page_size = self.config.get("forgetting_batch_size", 1000)  # æ¯æ‰¹å¤„ç†æ•°é‡

            logger.info(f"ğŸ“Š æ¸…ç†ä»»åŠ¡é…ç½®:")
            logger.info(f"  æ€»è®°å¿†æ•°: {total_memories}")
            logger.info(f"  ä¿ç•™å¤©æ•°: {retention_days}")
            logger.info(f"  è¡°å‡ç‡: {decay_rate}/å¤©")
            logger.info(f"  é‡è¦æ€§é˜ˆå€¼: {importance_threshold}")
            logger.info(f"  æ‰¹å¤„ç†å¤§å°: {page_size}")

            memories_to_update = []
            ids_to_delete = []
            total_processed = 0
            decay_count = 0

            # åˆ†é¡µå¤„ç†æ‰€æœ‰è®°å¿†
            batch_num = 0
            for offset in range(0, total_memories, page_size):
                batch_num += 1
                logger.debug(f"ğŸ“¦ å¤„ç†ç¬¬ {batch_num} æ‰¹ (offset={offset}, size={page_size})...")

                try:
                    batch_memories = await self.faiss_manager.get_memories_paginated(
                        page_size=page_size, offset=offset
                    )
                except Exception as e:
                    logger.error(f"âŒ è·å–ç¬¬ {batch_num} æ‰¹è®°å¿†å¤±è´¥: {e}", exc_info=True)
                    continue

                if not batch_memories:
                    logger.debug(f"  ç¬¬ {batch_num} æ‰¹æ— æ•°æ®ï¼Œç»“æŸåˆ†é¡µ")
                    break

                logger.debug(f"  ç¬¬ {batch_num} æ‰¹åŠ è½½äº† {len(batch_memories)} æ¡è®°å¿†")

                batch_updates = []
                batch_deletes = []

                for mem in batch_memories:
                    # ä½¿ç”¨ç»Ÿä¸€çš„å…ƒæ•°æ®è§£æå‡½æ•°
                    metadata = safe_parse_metadata(mem["metadata"])
                    if not metadata:
                        logger.warning(f"âš ï¸ è®°å¿† {mem['id']} çš„å…ƒæ•°æ®è§£æå¤±è´¥ï¼Œè·³è¿‡å¤„ç†")
                        continue

                    # 1. é‡è¦æ€§è¡°å‡
                    create_time = validate_timestamp(metadata.get("create_time"), current_time)
                    days_since_creation = (current_time - create_time) / (24 * 3600)

                    original_importance = metadata.get("importance", 0.5)
                    # çº¿æ€§è¡°å‡
                    decayed_importance = original_importance - (days_since_creation * decay_rate)
                    metadata["importance"] = max(0, decayed_importance)  # ç¡®ä¿ä¸ä¸ºè´Ÿ

                    if decayed_importance < original_importance:
                        decay_count += 1

                    mem["metadata"] = metadata  # æ›´æ–°å†…å­˜ä¸­çš„ metadata
                    batch_updates.append(mem)

                    # 2. è¯†åˆ«å¾…åˆ é™¤é¡¹
                    retention_seconds = retention_days * 24 * 3600
                    is_old = (current_time - create_time) > retention_seconds
                    is_unimportant = metadata["importance"] < importance_threshold

                    if is_old and is_unimportant:
                        batch_deletes.append(mem["id"])
                        logger.debug(
                            f"  æ ‡è®°åˆ é™¤: ID={mem['id']}, å¤©æ•°={days_since_creation:.1f}, "
                            f"é‡è¦æ€§={metadata['importance']:.3f}"
                        )

                # ç´¯ç§¯åˆ°å…¨å±€åˆ—è¡¨
                memories_to_update.extend(batch_updates)
                ids_to_delete.extend(batch_deletes)
                total_processed += len(batch_memories)

                # å¦‚æœæ‰¹æ¬¡æ•°æ®è¿‡å¤šï¼Œæ‰§è¡Œä¸­é—´æäº¤
                if len(memories_to_update) >= page_size * 2:
                    logger.debug(f"ğŸ’¾ æ‰§è¡Œä¸­é—´æ‰¹æ¬¡æ›´æ–°ï¼Œæ›´æ–° {len(memories_to_update)} æ¡è®°å¿†")
                    try:
                        await self.faiss_manager.update_memories_metadata(memories_to_update)
                        logger.debug(f"  ä¸­é—´æ‰¹æ¬¡æ›´æ–°æˆåŠŸ")
                    except Exception as e:
                        logger.error(f"âŒ ä¸­é—´æ‰¹æ¬¡æ›´æ–°å¤±è´¥: {e}", exc_info=True)
                    memories_to_update.clear()

                logger.debug(f"  æ‰¹æ¬¡è¿›åº¦: {total_processed}/{total_memories} ({(total_processed/total_memories)*100:.1f}%)")

            # 3. æ‰§è¡Œæœ€ç»ˆæ•°æ®åº“æ“ä½œ
            if memories_to_update:
                logger.info(f"ğŸ’¾ æ›´æ–° {len(memories_to_update)} æ¡è®°å¿†çš„é‡è¦æ€§å¾—åˆ†...")
                try:
                    await self.faiss_manager.update_memories_metadata(memories_to_update)
                    logger.info(f"  âœ… é‡è¦æ€§å¾—åˆ†æ›´æ–°æˆåŠŸ")
                except Exception as e:
                    logger.error(f"âŒ æ›´æ–°é‡è¦æ€§å¾—åˆ†å¤±è´¥: {e}", exc_info=True)

            if ids_to_delete:
                logger.info(f"ğŸ—‘ï¸ åˆ é™¤ {len(ids_to_delete)} æ¡é™ˆæ—§ä¸”ä¸é‡è¦çš„è®°å¿†...")
                # åˆ†æ‰¹åˆ é™¤ï¼Œé¿å…ä¸€æ¬¡åˆ é™¤å¤ªå¤š
                delete_batch_size = 100
                deleted_count = 0
                for i in range(0, len(ids_to_delete), delete_batch_size):
                    batch = ids_to_delete[i:i + delete_batch_size]
                    try:
                        await self.faiss_manager.delete_memories(batch)
                        deleted_count += len(batch)
                        logger.debug(f"  åˆ é™¤æ‰¹æ¬¡: {deleted_count}/{len(ids_to_delete)}")
                    except Exception as e:
                        logger.error(f"âŒ åˆ é™¤æ‰¹æ¬¡ {i//delete_batch_size + 1} å¤±è´¥: {e}", exc_info=True)

                logger.info(f"  âœ… æˆåŠŸåˆ é™¤ {deleted_count}/{len(ids_to_delete)} æ¡è®°å¿†")

            # æœ€ç»ˆç»Ÿè®¡
            logger.info(f"ğŸ“Š æ¸…ç†ä»»åŠ¡ç»Ÿè®¡:")
            logger.info(f"  å¤„ç†æ€»æ•°: {total_processed}")
            logger.info(f"  è¡°å‡æ•°é‡: {decay_count}")
            logger.info(f"  åˆ é™¤æ•°é‡: {len(ids_to_delete)}")
            logger.info(f"  å‰©ä½™è®°å¿†: {total_memories - len(ids_to_delete)}")

        except Exception as e:
            logger.error(
                f"âŒ è®°å¿†æ¸…ç†è¿‡ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {type(e).__name__}: {e}",
                exc_info=True
            )
