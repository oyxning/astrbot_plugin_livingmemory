# -*- coding: utf-8 -*-
"""
ç¨€ç–æ£€ç´¢å™¨ - åŸºäº SQLite FTS5 å’Œ BM25 çš„å…¨æ–‡æ£€ç´¢
"""

import json
import sqlite3
import math
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import asyncio
import aiosqlite

from astrbot.api import logger

try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    logger.warning("jieba not available, Chinese tokenization disabled")


@dataclass
class SparseResult:
    """ç¨€ç–æ£€ç´¢ç»“æœ"""
    doc_id: int
    score: float
    content: str
    metadata: Dict[str, Any]


class FTSManager:
    """FTS5 ç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.fts_table_name = "documents_fts"
        
    async def initialize(self):
        """åˆå§‹åŒ– FTS5 ç´¢å¼•"""
        async with aiosqlite.connect(self.db_path) as db:
            # å¯ç”¨ FTS5 æ‰©å±•
            await db.execute("PRAGMA foreign_keys = ON")
            
            # åˆ›å»º FTS5 è™šæ‹Ÿè¡¨
            await db.execute(f"""
                CREATE VIRTUAL TABLE IF NOT EXISTS {self.fts_table_name} 
                USING fts5(content, doc_id, tokenize='unicode61')
            """)
            
            # åˆ›å»ºè§¦å‘å™¨ï¼Œä¿æŒåŒæ­¥
            await self._create_triggers(db)
            
            await db.commit()
            logger.info(f"FTS5 index initialized: {self.fts_table_name}")
    
    async def _create_triggers(self, db: aiosqlite.Connection):
        """åˆ›å»ºæ•°æ®åŒæ­¥è§¦å‘å™¨"""
        # æ’å…¥è§¦å‘å™¨
        await db.execute(f"""
            CREATE TRIGGER IF NOT EXISTS documents_ai 
            AFTER INSERT ON documents BEGIN
                INSERT INTO {self.fts_table_name}(doc_id, content) 
                VALUES (new.id, new.text);
            END;
        """)
        
        # åˆ é™¤è§¦å‘å™¨
        await db.execute(f"""
            CREATE TRIGGER IF NOT EXISTS documents_ad 
            AFTER DELETE ON documents BEGIN
                DELETE FROM {self.fts_table_name} WHERE doc_id = old.id;
            END;
        """)
        
        # æ›´æ–°è§¦å‘å™¨
        await db.execute(f"""
            CREATE TRIGGER IF NOT EXISTS documents_au 
            AFTER UPDATE ON documents BEGIN
                DELETE FROM {self.fts_table_name} WHERE doc_id = old.id;
                INSERT INTO {self.fts_table_name}(doc_id, content) 
                VALUES (new.id, new.text);
            END;
        """)
    
    async def rebuild_index(self):
        """é‡å»ºç´¢å¼•"""
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute(f"DELETE FROM {self.fts_table_name}")
            await db.execute(f"""
                INSERT INTO {self.fts_table_name}(doc_id, content)
                SELECT id, text FROM documents
            """)
            await db.commit()
            logger.info("FTS index rebuilt")
    
    async def search(self, query: str, limit: int = 50) -> List[Tuple[int, float]]:
        """æ‰§è¡Œ BM25 æœç´¢"""
        async with aiosqlite.connect(self.db_path) as db:
            # å°†æ•´ä¸ªæŸ¥è¯¢ç”¨åŒå¼•å·åŒ…è£¹ï¼Œä»¥å¤„ç†ç‰¹æ®Šå­—ç¬¦å¹¶å°†å…¶ä½œä¸ºçŸ­è¯­æœç´¢
            # è¿™æ˜¯ä¸ºäº†é˜²æ­¢ FTS5 è¯­æ³•é”™è¯¯ï¼Œä¾‹å¦‚ 'syntax error near "."'
            safe_query = f'"{query}"'

            # ä½¿ç”¨ BM25 ç®—æ³•æœç´¢
            cursor = await db.execute(f"""
                SELECT doc_id, bm25({self.fts_table_name}) as score 
                FROM {self.fts_table_name} 
                WHERE {self.fts_table_name} MATCH ?
                ORDER BY score
                LIMIT ?
            """, (safe_query, limit))
            
            results = await cursor.fetchall()
            return [(row[0], row[1]) for row in results]


class SparseRetriever:
    """ç¨€ç–æ£€ç´¢å™¨"""
    
    def __init__(self, db_path: str, config: Dict[str, Any] = None):
        self.db_path = db_path
        self.config = config or {}
        self.fts_manager = FTSManager(db_path)
        self.enabled = self.config.get("enabled", True)
        self.use_chinese_tokenizer = self.config.get("use_chinese_tokenizer", JIEBA_AVAILABLE)

        logger.info("SparseRetriever åˆå§‹åŒ–")
        logger.info(f"  å¯ç”¨çŠ¶æ€: {'æ˜¯' if self.enabled else 'å¦'}")
        logger.info(f"  ä¸­æ–‡åˆ†è¯: {'æ˜¯' if self.use_chinese_tokenizer else 'å¦'} (jieba {'å¯ç”¨' if JIEBA_AVAILABLE else 'ä¸å¯ç”¨'})")
        logger.info(f"  æ•°æ®åº“è·¯å¾„: {db_path}")
        
    async def initialize(self):
        """åˆå§‹åŒ–ç¨€ç–æ£€ç´¢å™¨"""
        if not self.enabled:
            logger.info("ç¨€ç–æ£€ç´¢å™¨å·²ç¦ç”¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
            return

        logger.info("å¼€å§‹åˆå§‹åŒ–ç¨€ç–æ£€ç´¢å™¨...")

        try:
            await self.fts_manager.initialize()
            logger.info("âœ… FTS5 ç´¢å¼•åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ FTS5 ç´¢å¼•åˆå§‹åŒ–å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
            raise

        # å¦‚æœå¯ç”¨ä¸­æ–‡åˆ†è¯ï¼Œåˆå§‹åŒ– jieba
        if self.use_chinese_tokenizer and JIEBA_AVAILABLE:
            logger.debug("jieba ä¸­æ–‡åˆ†è¯å·²å¯ç”¨")
            # å¯ä»¥æ·»åŠ è‡ªå®šä¹‰è¯å…¸
            pass

        logger.info("âœ… ç¨€ç–æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _preprocess_query(self, query: str) -> str:
        """
        é¢„å¤„ç†æŸ¥è¯¢ï¼ŒåŒ…æ‹¬åˆ†è¯å’Œå®‰å…¨è½¬ä¹‰ã€‚

        Args:
            query: åŸå§‹æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            str: å¤„ç†åçš„å®‰å…¨æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        query = query.strip()

        # ä¸­æ–‡åˆ†è¯
        if self.use_chinese_tokenizer and JIEBA_AVAILABLE:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡
            if any('\u4e00' <= char <= '\u9fff' for char in query):
                tokens = jieba.cut_for_search(query)
                query = " ".join(tokens)

        # FTS5 å®‰å…¨è½¬ä¹‰: åŒå¼•å·éœ€è¦è½¬ä¹‰ä¸ºä¸¤ä¸ªåŒå¼•å·
        # ç§»é™¤å¯èƒ½å¯¼è‡´è¯­æ³•é”™è¯¯çš„ç‰¹æ®ŠFTS5æ“ä½œç¬¦
        query = query.replace('"', '""')  # FTS5è½¬ä¹‰è§„åˆ™

        # ç§»é™¤å¯èƒ½çš„FTS5ç‰¹æ®Šå­—ç¬¦å’Œæ“ä½œç¬¦
        # FTS5ç‰¹æ®Šå­—ç¬¦: * (é€šé…ç¬¦), ^ (åˆ—è¿‡æ»¤), NEAR, AND, OR, NOT
        # ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬å°†æŸ¥è¯¢ä½œä¸ºçŸ­è¯­æœç´¢ï¼Œç¦ç”¨è¿™äº›æ“ä½œç¬¦
        query = query.replace('*', ' ')  # ç§»é™¤é€šé…ç¬¦
        query = query.replace('^', ' ')  # ç§»é™¤åˆ—è¿‡æ»¤ç¬¦

        return query
    
    async def search(
        self,
        query: str,
        limit: int = 50,
        session_id: Optional[str] = None,
        persona_id: Optional[str] = None,
        metadata_filters: Optional[Dict[str, Any]] = None
    ) -> List[SparseResult]:
        """æ‰§è¡Œç¨€ç–æ£€ç´¢"""
        if not self.enabled:
            logger.debug("ç¨€ç–æ£€ç´¢å™¨æœªå¯ç”¨ï¼Œè¿”å›ç©ºç»“æœ")
            return []

        logger.debug(f"ç¨€ç–æ£€ç´¢: query='{query[:50]}...', limit={limit}")

        try:
            # é¢„å¤„ç†æŸ¥è¯¢
            processed_query = self._preprocess_query(query)
            logger.debug(f"  åŸå§‹æŸ¥è¯¢: '{query[:50]}...'")
            logger.debug(f"  å¤„ç†åæŸ¥è¯¢: '{processed_query[:50]}...'")

            # æ‰§è¡Œ FTS æœç´¢
            fts_results = await self.fts_manager.search(processed_query, limit)

            if not fts_results:
                logger.debug("  FTS æœç´¢æ— ç»“æœ")
                return []

            logger.debug(f"  FTS è¿”å› {len(fts_results)} æ¡åŸå§‹ç»“æœ")

            # è·å–å®Œæ•´çš„æ–‡æ¡£ä¿¡æ¯
            doc_ids = [doc_id for doc_id, _ in fts_results]
            logger.debug(f"  è·å–æ–‡æ¡£è¯¦æƒ…: {len(doc_ids)} ä¸ª ID")
            documents = await self._get_documents(doc_ids)
            logger.debug(f"  æˆåŠŸè·å– {len(documents)} ä¸ªæ–‡æ¡£")

            # åº”ç”¨è¿‡æ»¤å™¨
            filtered_results = []
            for doc_id, bm25_score in fts_results:
                if doc_id in documents:
                    doc = documents[doc_id]

                    # æ£€æŸ¥å…ƒæ•°æ®è¿‡æ»¤å™¨
                    if self._apply_filters(doc.get("metadata", {}), session_id, persona_id, metadata_filters):
                        result = SparseResult(
                            doc_id=doc_id,
                            score=bm25_score,
                            content=doc["text"],
                            metadata=doc["metadata"]
                        )
                        filtered_results.append(result)

            logger.debug(f"  è¿‡æ»¤åå‰©ä½™ {len(filtered_results)} æ¡ç»“æœ")

            # å½’ä¸€åŒ– BM25 åˆ†æ•°ï¼ˆè½¬æ¢ä¸º 0-1ï¼‰
            if filtered_results:
                max_score = max(r.score for r in filtered_results)
                min_score = min(r.score for r in filtered_results)
                score_range = max_score - min_score if max_score != min_score else 1

                logger.debug(f"  å½’ä¸€åŒ–åˆ†æ•°: min={min_score:.3f}, max={max_score:.3f}, range={score_range:.3f}")

                for result in filtered_results:
                    original_score = result.score
                    result.score = (result.score - min_score) / score_range
                    logger.debug(f"    ID={result.doc_id}: {original_score:.3f} -> {result.score:.3f}")

            logger.info(f"âœ… ç¨€ç–æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(filtered_results)} æ¡ç»“æœ")
            return filtered_results

        except Exception as e:
            logger.error(
                f"âŒ ç¨€ç–æ£€ç´¢å¤±è´¥: {type(e).__name__}: {e}",
                exc_info=True
            )
            logger.error(f"  å¤±è´¥ä¸Šä¸‹æ–‡: query='{query[:50]}...', limit={limit}")
            return []
    
    async def _get_documents(self, doc_ids: List[int]) -> Dict[int, Dict[str, Any]]:
        """æ‰¹é‡è·å–æ–‡æ¡£"""
        async with aiosqlite.connect(self.db_path) as db:
            placeholders = ",".join("?" for _ in doc_ids)
            cursor = await db.execute(f"""
                SELECT id, text, metadata FROM documents WHERE id IN ({placeholders})
            """, doc_ids)
            
            documents = {}
            async for row in cursor:
                metadata = json.loads(row[2]) if isinstance(row[2], str) else row[2]
                documents[row[0]] = {
                    "text": row[1],
                    "metadata": metadata or {}
                }
            
            return documents
    
    def _apply_filters(
        self, 
        metadata: Dict[str, Any], 
        session_id: Optional[str],
        persona_id: Optional[str],
        metadata_filters: Optional[Dict[str, Any]]
    ) -> bool:
        """åº”ç”¨è¿‡æ»¤å™¨"""
        # ä¼šè¯è¿‡æ»¤
        if session_id and metadata.get("session_id") != session_id:
            return False
        
        # äººæ ¼è¿‡æ»¤
        if persona_id and metadata.get("persona_id") != persona_id:
            return False
        
        # è‡ªå®šä¹‰å…ƒæ•°æ®è¿‡æ»¤
        if metadata_filters:
            for key, value in metadata_filters.items():
                if metadata.get(key) != value:
                    return False
        
        return True
    
    async def rebuild_index(self):
        """é‡å»ºç´¢å¼•"""
        if not self.enabled:
            logger.warning("ç¨€ç–æ£€ç´¢å™¨æœªå¯ç”¨ï¼Œæ— æ³•é‡å»ºç´¢å¼•")
            return

        logger.info("ğŸ”„ å¼€å§‹é‡å»º FTS5 ç´¢å¼•...")

        try:
            await self.fts_manager.rebuild_index()
            logger.info("âœ… FTS5 ç´¢å¼•é‡å»ºæˆåŠŸ")
        except Exception as e:
            logger.error(
                f"âŒ é‡å»º FTS5 ç´¢å¼•å¤±è´¥: {type(e).__name__}: {e}",
                exc_info=True
            )
            raise