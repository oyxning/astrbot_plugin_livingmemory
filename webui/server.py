# -*- coding: utf-8 -*-
"""
server.py - LivingMemory WebUI backend
Provides authentication, memory browsing, detail view and bulk deletion APIs built on FastAPI.
"""


import asyncio
import json
import secrets
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional, Tuple,List, TYPE_CHECKING

import uvicorn
from fastapi import Depends, FastAPI, HTTPException, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles

from astrbot.api import logger

from ..core.utils import safe_parse_metadata
from ..storage.memory_storage import MemoryStorage

if TYPE_CHECKING:
    from ..storage.faiss_manager import FaissManager
    from ..main import SessionManager


class WebUIServer:
    """
    Helper class responsible for starting and managing the LivingMemory WebUI service.
    """

    def __init__(
        self,
        config: Dict[str, Any],
        faiss_manager: "FaissManager",
        session_manager: Optional["SessionManager"] = None,
    ):
        self.config = config
        self.faiss_manager = faiss_manager
        self.session_manager = session_manager

        self.host = str(config.get("host", "127.0.0.1"))
        self.port = int(config.get("port", 8080))
        self.session_timeout = max(60, int(config.get("session_timeout", 3600)))
        self._access_password = str(config.get("access_password", "")).strip()

        # Token ç®¡ç† - ä¿®å¤: ä½¿ç”¨å­—å…¸å­˜å‚¨æ›´å¤šä¿¡æ¯é˜²æ­¢æ°¸ä¸è¿‡æœŸ
        self._tokens: Dict[str, Dict[str, float]] = {}
        self._token_lock = asyncio.Lock()

        # è¯·æ±‚é¢‘ç‡é™åˆ¶ - æ–°å¢: é˜²æ­¢æš´åŠ›ç ´è§£
        self._failed_attempts: Dict[str, List[float]] = {}
        self._attempt_lock = asyncio.Lock()

        self._server: Optional[uvicorn.Server] = None
        self._server_task: Optional[asyncio.Task] = None
        self._cleanup_task: Optional[asyncio.Task] = None
        self._nuke_task: Optional[asyncio.Task] = None

        self.memory_storage: Optional[MemoryStorage] = None
        self._storage_prepared = False
        self._pending_nuke: Optional[Dict[str, Any]] = None
        self._nuke_lock = asyncio.Lock()

        self._app = FastAPI(title="LivingMemory æ§åˆ¶å°", version="1.3.3")
        self._setup_routes()

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    async def start(self):
        """
        å¯åŠ¨ WebUI æœåŠ¡ã€‚
        """
        if self._server_task and not self._server_task.done():
            logger.warning("WebUI æœåŠ¡å·²ç»åœ¨è¿è¡Œ")
            return

        await self._prepare_storage()

        config = uvicorn.Config(
            app=self._app,
            host=self.host,
            port=self.port,
            log_level="info",
            loop="asyncio",
            lifespan="on",
        )
        self._server = uvicorn.Server(config)
        self._server_task = asyncio.create_task(self._server.serve())

        # å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡ - æ–°å¢: é˜²æ­¢å†…å­˜æ³„æ¼
        self._cleanup_task = asyncio.create_task(self._periodic_cleanup())

        # ç­‰å¾…æœåŠ¡å¯åŠ¨
        for _ in range(50):
            if getattr(self._server, "started", False):
                logger.info(f"WebUI å·²å¯åŠ¨: http://{self.host}:{self.port}")
                return
            if self._server_task.done():
                error = self._server_task.exception()
                raise RuntimeError(f"WebUI å¯åŠ¨å¤±è´¥: {error}") from error
            await asyncio.sleep(0.1)

        logger.warning("WebUI å¯åŠ¨è€—æ—¶è¾ƒé•¿ï¼Œä»åœ¨åå°å¯åŠ¨ä¸­")

    async def stop(self):
        """
        åœæ­¢ WebUI æœåŠ¡ã€‚
        """
        # åœæ­¢å®šæœŸæ¸…ç†ä»»åŠ¡
        if self._cleanup_task and not self._cleanup_task.done():
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass

        if self._server:
            self._server.should_exit = True
        if self._server_task:
            await self._server_task
        self._server = None
        self._server_task = None
        if self._nuke_task and not self._nuke_task.done():
            self._nuke_task.cancel()
            try:
                await self._nuke_task
            except asyncio.CancelledError:
                pass
        self._nuke_task = None
        self._pending_nuke = None
        self._cleanup_task = None
        logger.info("WebUI å·²åœæ­¢")

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    async def _periodic_cleanup(self):
        """
        å®šæœŸæ¸…ç†è¿‡æœŸ token å’Œå¤±è´¥å°è¯•è®°å½• - æ–°å¢: é˜²æ­¢å†…å­˜æ³„æ¼
        """
        while True:
            try:
                await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                async with self._token_lock:
                    await self._cleanup_tokens_locked()
                async with self._attempt_lock:
                    await self._cleanup_failed_attempts_locked()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"å®šæœŸæ¸…ç†ä»»åŠ¡å‡ºé”™: {e}")

    async def _cleanup_failed_attempts_locked(self):
        """
        æ¸…ç†è¿‡æœŸçš„å¤±è´¥å°è¯•è®°å½• - æ–°å¢
        """
        now = time.time()
        expired_ips = []
        for ip, attempts in self._failed_attempts.items():
            # åªä¿ç•™5åˆ†é’Ÿå†…çš„å°è¯•è®°å½•
            recent = [t for t in attempts if now - t < 300]
            if recent:
                self._failed_attempts[ip] = recent
            else:
                expired_ips.append(ip)

        for ip in expired_ips:
            self._failed_attempts.pop(ip, None)

    async def _check_rate_limit(self, client_ip: str) -> bool:
        """
        æ£€æŸ¥è¯·æ±‚é¢‘ç‡é™åˆ¶ - æ–°å¢: é˜²æ­¢æš´åŠ›ç ´è§£

        Returns:
            bool: True è¡¨ç¤ºæœªè¶…é™, False è¡¨ç¤ºå·²è¶…é™
        """
        async with self._attempt_lock:
            await self._cleanup_failed_attempts_locked()
            attempts = self._failed_attempts.get(client_ip, [])
            recent = [t for t in attempts if time.time() - t < 300]  # 5åˆ†é’Ÿçª—å£

            if len(recent) >= 5:  # 5åˆ†é’Ÿå†…æœ€å¤š5æ¬¡å¤±è´¥å°è¯•
                return False
            return True

    async def _record_failed_attempt(self, client_ip: str):
        """
        è®°å½•å¤±è´¥çš„ç™»å½•å°è¯• - æ–°å¢
        """
        async with self._attempt_lock:
            if client_ip not in self._failed_attempts:
                self._failed_attempts[client_ip] = []
            self._failed_attempts[client_ip].append(time.time())

    async def _prepare_storage(self):
        """
        åˆå§‹åŒ–è‡ªå®šä¹‰è®°å¿†å­˜å‚¨ï¼ˆå¦‚å¯ç”¨ï¼‰ã€‚
        """
        if self._storage_prepared:
            return

        connection = None
        try:
            doc_storage = getattr(self.faiss_manager.db, "document_storage", None)
            connection = getattr(doc_storage, "connection", None)
        except Exception as exc:  # pragma: no cover
            logger.debug(f"è·å–æ–‡æ¡£å­˜å‚¨è¿æ¥å¤±è´¥: {exc}")

        if connection:
            try:
                storage = MemoryStorage(connection)
                await storage.initialize_schema()
                self.memory_storage = storage
                logger.info("WebUI å·²æ¥å…¥æ’ä»¶è‡ªå®šä¹‰çš„è®°å¿†å­˜å‚¨ï¼ˆSQLiteï¼‰")
            except Exception as exc:
                logger.warning(f"åˆå§‹åŒ– MemoryStorage å¤±è´¥ï¼Œå°†å›é€€è‡³æ–‡æ¡£å­˜å‚¨: {exc}")
                self.memory_storage = None
        else:
            logger.debug("æœªè·å–åˆ° MemoryStorage è¿æ¥ï¼Œå°†ä»…ä½¿ç”¨ Faiss æ–‡æ¡£å­˜å‚¨æ¥å£")

        self._storage_prepared = True

    def _setup_routes(self):
        """
        åˆå§‹åŒ– FastAPI è·¯ç”±ä¸é™æ€èµ„æºã€‚
        """
        static_dir = Path(__file__).resolve().parent.parent / "static"
        index_path = static_dir / "index.html"
        if not index_path.exists():
            logger.warning("æœªæ‰¾åˆ° WebUI å‰ç«¯æ–‡ä»¶ï¼Œé™æ€èµ„æºç›®å½•ä¸ºç©º")

        self._app.add_middleware(
            CORSMiddleware,
            allow_origins=[
                f"http://{self.host}:{self.port}",
                "http://localhost",
                "http://127.0.0.1",
            ],
            allow_methods=["GET", "POST", "DELETE"],  # ä¿®å¤: é™åˆ¶å…è®¸çš„æ–¹æ³•
            allow_headers=["Content-Type", "Authorization", "X-Auth-Token"],  # ä¿®å¤: é™åˆ¶å…è®¸çš„å¤´éƒ¨
            allow_credentials=True,
        )

        self._app.mount("/static", StaticFiles(directory=static_dir), name="static")

        @self._app.get("/", response_class=HTMLResponse)
        async def serve_index():
            if not index_path.exists():
                raise HTTPException(status.HTTP_404_NOT_FOUND, detail="å‰ç«¯æ–‡ä»¶ç¼ºå¤±")
            return HTMLResponse(index_path.read_text(encoding="utf-8"))

        @self._app.post("/api/login")
        async def login(request: Request, payload: Dict[str, Any]):
            password = str(payload.get("password", "")).strip()
            if not password:
                raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="å¯†ç ä¸èƒ½ä¸ºç©º")

            # æ£€æŸ¥è¯·æ±‚é¢‘ç‡é™åˆ¶ - æ–°å¢
            client_ip = request.client.host if request.client else "unknown"
            if not await self._check_rate_limit(client_ip):
                raise HTTPException(
                    status.HTTP_429_TOO_MANY_REQUESTS,
                    detail="å°è¯•æ¬¡æ•°è¿‡å¤šï¼Œè¯·5åˆ†é’Ÿåå†è¯•"
                )

            if password != self._access_password:
                # è®°å½•å¤±è´¥å°è¯• - æ–°å¢
                await self._record_failed_attempt(client_ip)
                await asyncio.sleep(1.0)  # å¢åŠ å»¶è¿Ÿåˆ°1ç§’ï¼Œå‡ç¼“æš´åŠ›ç ´è§£
                raise HTTPException(status.HTTP_401_UNAUTHORIZED, detail="è®¤è¯å¤±è´¥")

            # ç”Ÿæˆ token - ä¿®å¤: ä½¿ç”¨å­—å…¸å­˜å‚¨å¤šä¸ªæ—¶é—´æˆ³
            token = secrets.token_urlsafe(32)
            now = time.time()
            max_lifetime = 86400  # 24å°æ—¶ç»å¯¹è¿‡æœŸ

            async with self._token_lock:
                await self._cleanup_tokens_locked()
                self._tokens[token] = {
                    "created_at": now,
                    "last_active": now,
                    "max_lifetime": max_lifetime
                }

            return {"token": token, "expires_in": self.session_timeout}

        @self._app.post("/api/logout")
        async def logout(token: str = Depends(self._auth_dependency())):
            async with self._token_lock:
                self._tokens.pop(token, None)
            return {"detail": "å·²é€€å‡ºç™»å½•"}

        @self._app.get("/api/memories")
        async def list_memories(
            request: Request,
            token: str = Depends(self._auth_dependency()),
        ):
            query = request.query_params
            keyword = query.get("keyword", "").strip()
            status_filter = query.get("status", "all").strip() or "all"
            load_all = query.get("all", "false").lower() == "true"

            if load_all:
                page = 1
                page_size = 0
                offset = 0
            else:
                page = max(1, int(query.get("page", 1)))
                page_size = query.get("page_size")
                page_size = min(200, max(1, int(page_size))) if page_size else 50
                offset = (page - 1) * page_size

            try:
                total, items = await self._fetch_memories(
                    page=page,
                    page_size=page_size,
                    offset=offset,
                    status_filter=status_filter,
                    keyword=keyword,
                    load_all=load_all,
                )
            except Exception as exc:
                logger.error(f"è·å–è®°å¿†åˆ—è¡¨å¤±è´¥: {exc}", exc_info=True)
                raise HTTPException(
                    status.HTTP_500_INTERNAL_SERVER_ERROR, detail="è¯»å–è®°å¿†å¤±è´¥"
                ) from exc

            has_more = False if load_all else offset + len(items) < total
            effective_page_size = page_size if page_size else len(items)

            return {
                "items": items,
                "page": page,
                "page_size": effective_page_size,
                "total": total,
                "has_more": has_more,
            }

        @self._app.get("/api/memories/{memory_id}")
        async def memory_detail(
            memory_id: str, token: str = Depends(self._auth_dependency())
        ):
            detail = await self._get_memory_detail(memory_id)
            if not detail:
                raise HTTPException(status.HTTP_404_NOT_FOUND, detail="æœªæ‰¾åˆ°è®°å¿†è®°å½•")
            return detail

        @self._app.delete("/api/memories")
        async def delete_memories(
            payload: Dict[str, Any],
            token: str = Depends(self._auth_dependency()),
        ):
            doc_ids = payload.get("doc_ids") or payload.get("ids") or []
            memory_ids = payload.get("memory_ids") or []

            if not doc_ids and not memory_ids:
                raise HTTPException(
                    status.HTTP_400_BAD_REQUEST, detail="éœ€è¦æä¾›å¾…åˆ é™¤çš„è®°å¿†IDåˆ—è¡¨"
                )

            deleted_docs = 0
            deleted_memories = 0

            if doc_ids:
                try:
                    doc_ids_int = [int(x) for x in doc_ids]
                    await self.faiss_manager.delete_memories(doc_ids_int)
                    deleted_docs = len(doc_ids_int)
                except Exception as exc:
                    logger.error(f"åˆ é™¤ Faiss è®°å¿†å¤±è´¥: {exc}", exc_info=True)
                    raise HTTPException(
                        status.HTTP_500_INTERNAL_SERVER_ERROR, detail="å‘é‡è®°å¿†åˆ é™¤å¤±è´¥"
                    ) from exc

            if memory_ids and self.memory_storage:
                try:
                    ids = [str(x) for x in memory_ids]
                    await self.memory_storage.delete_memories_by_memory_ids(ids)
                    deleted_memories = len(ids)
                except Exception as exc:
                    logger.error(f"åˆ é™¤ç»“æ„åŒ–è®°å¿†å¤±è´¥: {exc}", exc_info=True)
                    raise HTTPException(
                        status.HTTP_500_INTERNAL_SERVER_ERROR, detail="ç»“æ„åŒ–è®°å¿†åˆ é™¤å¤±è´¥"
                    ) from exc

            return {
                "deleted_doc_count": deleted_docs,
                "deleted_memory_count": deleted_memories,
            }

        @self._app.post("/api/memories/nuke")
        async def schedule_memory_nuke(
            payload: Optional[Dict[str, Any]] = None,
            token: str = Depends(self._auth_dependency()),
        ):
            delay = 30
            if payload and "delay" in payload:
                try:
                    delay = int(payload["delay"])
                except (TypeError, ValueError):
                    raise HTTPException(
                        status.HTTP_400_BAD_REQUEST, detail="delay å‚æ•°æ— æ•ˆ"
                    )
            return await self._schedule_nuke(delay)

        @self._app.get("/api/memories/nuke")
        async def get_memory_nuke_status(
            token: str = Depends(self._auth_dependency()),
        ):
            return await self._get_pending_nuke()

        @self._app.delete("/api/memories/nuke/{operation_id}")
        async def cancel_memory_nuke(
            operation_id: str,
            token: str = Depends(self._auth_dependency()),
        ):
            cancelled = await self._cancel_nuke(operation_id)
            if not cancelled:
                raise HTTPException(
                    status.HTTP_404_NOT_FOUND, detail="å½“å‰æ²¡æœ‰åŒ¹é…çš„æ ¸çˆ†ä»»åŠ¡"
                )
            return {"detail": "å·²å–æ¶ˆæ ¸çˆ†ä»»åŠ¡", "operation_id": operation_id}

        @self._app.get("/api/stats")
        async def stats(token: str = Depends(self._auth_dependency())):
            total, status_counts = await self._gather_statistics()
            active_sessions = (
                self.session_manager.get_session_count()
                if self.session_manager
                else 0
            )

            return {
                "total_memories": total,
                "status_breakdown": status_counts,
                "active_sessions": active_sessions,
                "session_timeout": self.session_timeout,
            }

        @self._app.get("/api/health")
        async def health():
            return {"status": "ok"}

    async def _fetch_memories(
        self,
        page: int,
        page_size: int,
        offset: int,
        status_filter: str,
        keyword: str,
        load_all: bool,
    ) -> Tuple[int, list]:
        try:
            total, records = await self._query_faiss_memories(
                offset=offset,
                page_size=page_size,
                status_filter=status_filter,
                keyword=keyword,
                load_all=load_all,
            )
        except Exception as exc:
            logger.error(f"ä½¿ç”¨ä¼˜åŒ–æŸ¥è¯¢è·å–è®°å¿†å¤±è´¥ï¼Œå°†å›é€€åŸºç¡€å®ç°: {exc}", exc_info=True)
            total, records = await self._fetch_memories_fallback(
                offset=offset,
                page_size=page_size,
                status_filter=status_filter,
                keyword=keyword,
                load_all=load_all,
            )

        items = [self._format_memory(record, source="faiss") for record in records]
        return total, items

    async def _query_faiss_memories(
        self,
        offset: int,
        page_size: int,
        status_filter: str,
        keyword: str,
        load_all: bool,
    ) -> Tuple[int, List[Dict[str, Any]]]:
        doc_storage = getattr(self.faiss_manager.db, "document_storage", None)
        connection = getattr(doc_storage, "connection", None)
        if connection is None:
            raise RuntimeError("Document storage connection unavailable")

        conditions: List[str] = []
        params: List[Any] = []

        status_value = (status_filter or "").strip().lower()
        if status_value and status_value != "all":
            conditions.append("LOWER(COALESCE(json_extract(metadata, '$.status'), 'active')) = ?")
            params.append(status_value)

        keyword_value = (keyword or "").strip()
        if keyword_value:
            keyword_param = f"%{keyword_value.lower()}%"
            conditions.append("("
                "LOWER(text) LIKE ? OR "
                "LOWER(COALESCE(json_extract(metadata, '$.memory_content'), '')) LIKE ? OR "
                "LOWER(COALESCE(json_extract(metadata, '$.memory_id'), '')) LIKE ?"
                ")")
            params.extend([keyword_param, keyword_param, keyword_param])

        where_clause = f" WHERE {' AND '.join(conditions)}" if conditions else ""

        count_sql = f"SELECT COUNT(*) FROM documents{where_clause}"
        async with connection.execute(count_sql, params) as cursor:
            row = await cursor.fetchone()
        total = int(row[0]) if row and row[0] is not None else 0

        query_sql = (
            "SELECT id, text, metadata FROM documents"
            f"{where_clause} ORDER BY id DESC"
        )
        query_params = list(params)
        if not load_all and page_size > 0:
            query_sql += " LIMIT ? OFFSET ?"
            query_params.extend([page_size, offset])

        async with connection.execute(query_sql, query_params) as cursor:
            rows = await cursor.fetchall()

        records: List[Dict[str, Any]] = []
        for row in rows:
            metadata_raw = row[2]
            if isinstance(metadata_raw, str):
                try:
                    metadata = json.loads(metadata_raw)
                except json.JSONDecodeError:
                    metadata = {}
            else:
                metadata = metadata_raw or {}

            records.append(
                {
                    "id": row[0],
                    "content": row[1],
                    "metadata": metadata,
                }
            )

        return total, records

    async def _fetch_memories_fallback(
        self,
        offset: int,
        page_size: int,
        status_filter: str,
        keyword: str,
        load_all: bool,
    ) -> Tuple[int, List[Dict[str, Any]]]:
        total_available = await self.faiss_manager.count_total_memories()
        fetch_size = max(total_available, page_size if page_size else 0, 1)

        records = await self.faiss_manager.get_memories_paginated(
            page_size=fetch_size, offset=0
        )

        filtered_records = self._filter_records(records, status_filter, keyword)
        total_filtered = len(filtered_records)

        if load_all:
            return total_filtered, filtered_records

        start = max(0, offset)
        end = start + page_size if page_size else total_filtered
        return total_filtered, filtered_records[start:end]

    def _filter_records(
        self,
        records: List[Dict[str, Any]],
        status_filter: str,
        keyword: str
    ) -> List[Dict[str, Any]]:
        """
        åœ¨å†…å­˜ä¸­è¿‡æ»¤è®°å½• - æ–°å¢: æ”¯æŒçŠ¶æ€å’Œå…³é”®è¯ç­›é€‰
        """
        filtered = []

        for record in records:
            # metadata ç°åœ¨å·²ç»æ˜¯å­—å…¸
            metadata = record.get("metadata", {})

            # çŠ¶æ€è¿‡æ»¤
            if status_filter and status_filter != "all":
                record_status = metadata.get("status", "active")
                if record_status != status_filter:
                    continue

            # å…³é”®è¯è¿‡æ»¤ (æœç´¢ content å’Œ memory_content)
            if keyword:
                content = record.get("content", "")
                memory_content = metadata.get("memory_content", "")
                keyword_lower = keyword.lower()

                if (keyword_lower not in content.lower() and
                    keyword_lower not in memory_content.lower()):
                    continue

            filtered.append(record)

        return filtered

    async def _get_memory_detail(self, memory_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–å•ä¸ªè®°å¿†è¯¦æƒ… - ä¿®å¤: ç»Ÿä¸€ä½¿ç”¨ Faiss æ–‡æ¡£å­˜å‚¨
        """
        # å°è¯•æŒ‰æ–‡æ¡£IDæŸ¥è¯¢
        try:
            doc_id = int(memory_id)
        except ValueError:
            # å¦‚æœä¸æ˜¯æ•´æ•°,å°è¯•æŒ‰ memory_id æŸ¥è¯¢
            doc_id = None

        try:
            if doc_id is not None:
                # æŒ‰æ•´æ•° ID æŸ¥è¯¢
                docs = await self.faiss_manager.db.document_storage.get_documents(
                    ids=[doc_id]
                )
            else:
                # æŒ‰ memory_id æŸ¥è¯¢ (åœ¨ metadata ä¸­)
                all_docs = await self.faiss_manager.get_memories_paginated(
                    page_size=10000, offset=0
                )
                docs = [
                    doc for doc in all_docs
                    if doc.get("metadata", {}).get("memory_id") == memory_id
                ]

            if not docs:
                return None

            # metadata å·²ç»æ˜¯å­—å…¸,ç›´æ¥è¿”å›
            return self._format_memory(docs[0], source="faiss")

        except Exception as exc:
            logger.error(f"æŸ¥è¯¢è®°å¿†è¯¦æƒ…å¤±è´¥: {exc}", exc_info=True)
            return None

    def _format_memory(self, raw: Dict[str, Any], source: str) -> Dict[str, Any]:
        if source == "storage":
            memory_json = raw.get("memory_data") or "{}"
            parsed = self._safe_json_loads(memory_json)
            metadata = parsed.get("metadata", {})
            access_info = metadata.get("access_info", {})

            summary = (
                parsed.get("summary")
                or parsed.get("description")
                or parsed.get("memory_content")
                or ""
            )
            created_at = parsed.get("timestamp") or raw.get("timestamp")
            last_access = access_info.get("last_accessed_timestamp")

            return {
                "doc_id": None,
                "memory_id": raw.get("memory_id"),
                "summary": summary,
                "memory_type": raw.get("memory_type"),
                "importance": raw.get("importance_score"),
                "status": raw.get("status"),
                "created_at": self._format_timestamp(created_at),
                "last_access": self._format_timestamp(last_access),
                "source": "storage",
                "metadata": metadata,
                "raw": parsed,
                "raw_json": memory_json,
            }

        # Faiss source çš„æ ¼å¼åŒ– (ä¿®å¤: metadata ç°åœ¨å·²ç»æ˜¯å­—å…¸)
        metadata = raw.get("metadata", {})  # âœ… å·²ç»æ˜¯å­—å…¸,ä¸éœ€è¦ safe_parse_metadata

        # ä¼˜å…ˆä½¿ç”¨ metadata.memory_content,fallback åˆ° content
        summary = metadata.get("memory_content") or raw.get("content") or ""
        importance = metadata.get("importance")
        event_type = metadata.get("event_type")
        status = metadata.get("status", "active")
        created_at = metadata.get("create_time")
        last_access = metadata.get("last_access_time")

        return {
            "doc_id": raw.get("id"),
            "memory_id": metadata.get("memory_id"),
            "summary": summary,
            "memory_type": event_type,
            "importance": importance,
            "status": status,
            "created_at": self._format_timestamp(created_at),
            "last_access": self._format_timestamp(last_access),
            "source": "faiss",
            "metadata": metadata,
            "raw": {
                "content": raw.get("content"),
                "metadata": metadata,
            },
            "raw_json": json.dumps(metadata, ensure_ascii=False),
        }

    async def _gather_statistics(self) -> Tuple[int, Dict[str, int]]:
        """
        ç»Ÿè®¡è®°å¿†æ•°é‡ - ä¿®å¤: ç»Ÿä¸€ä½¿ç”¨ Faiss æ–‡æ¡£å­˜å‚¨
        """
        total = await self.faiss_manager.count_total_memories()
        counts = await self._collect_status_counts()
        return total, counts

    async def _collect_status_counts(self) -> Dict[str, int]:
        """
        é’ˆå¯¹ Faiss æ–‡æ¡£å­˜å‚¨ç»Ÿè®¡ä¸åŒçŠ¶æ€çš„è®°å¿†æ•°é‡ã€‚
        """
        counts: Dict[str, int] = {"active": 0, "archived": 0, "deleted": 0}
        try:
            conn = self.faiss_manager.db.document_storage.connection
            async with conn.execute(
                "SELECT json_extract(metadata, '$.status') AS status FROM documents"
            ) as cursor:
                rows = await cursor.fetchall()
            for row in rows:
                status_value = row[0] if row and row[0] else "active"
                counts[status_value] = counts.get(status_value, 0) + 1
        except Exception as exc:
            logger.error(f"ç»Ÿè®¡è®°å¿†çŠ¶æ€å¤±è´¥: {exc}", exc_info=True)
        return counts

    def _serialize_nuke_status(
        self,
        payload: Optional[Dict[str, Any]],
        now: Optional[float] = None,
        already_pending: bool = False,
    ) -> Dict[str, Any]:
        if not payload:
            return {"pending": False}

        now = now or time.time()
        execute_at = float(payload.get("execute_at", now))
        seconds_left = max(0, int(round(execute_at - now)))
        if already_pending:
            detail = "A pending wipe is already counting down"
        else:
            detail = (
                f"Wipe executes in {seconds_left} seconds"
                if seconds_left
                else "Wipe executing now"
            )

        return {
            "pending": True,
            "operation_id": payload.get("id"),
            "execute_at": datetime.fromtimestamp(execute_at).isoformat(
                sep=" ", timespec="seconds"
            ),
            "seconds_left": seconds_left,
            "detail": detail,
            "already_pending": already_pending,
        }

    async def _schedule_nuke(self, delay_seconds: int) -> Dict[str, Any]:
        delay = max(5, min(int(delay_seconds), 600))
        task_to_cancel: Optional[asyncio.Task] = None
        pending_snapshot: Dict[str, Any]

        async with self._nuke_lock:
            now = time.time()
            if self._pending_nuke and self._pending_nuke.get("status") == "scheduled":
                return self._serialize_nuke_status(self._pending_nuke, now, True)

            if self._nuke_task and not self._nuke_task.done():
                task_to_cancel = self._nuke_task

            operation_id = secrets.token_urlsafe(8)
            execute_at = now + delay
            pending = {
                "id": operation_id,
                "created_at": now,
                "execute_at": execute_at,
                "status": "scheduled",
            }
            self._pending_nuke = pending
            self._nuke_task = asyncio.create_task(self._run_nuke(operation_id, delay))
            pending_snapshot = dict(pending)

        if task_to_cancel:
            task_to_cancel.cancel()
            try:
                await task_to_cancel
            except asyncio.CancelledError:
                pass

        return self._serialize_nuke_status(pending_snapshot, time.time())

    async def _get_pending_nuke(self) -> Dict[str, Any]:
        async with self._nuke_lock:
            pending = self._pending_nuke
            if not pending or pending.get("status") != "scheduled":
                return {"pending": False}
            snapshot = dict(pending)
        return self._serialize_nuke_status(snapshot)

    async def _cancel_nuke(self, operation_id: str) -> bool:
        task: Optional[asyncio.Task] = None
        async with self._nuke_lock:
            if not self._pending_nuke or self._pending_nuke.get("id") != operation_id:
                return False
            task = self._nuke_task
            self._pending_nuke = None
            self._nuke_task = None

        if task and not task.done():
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
        return True

    async def _run_nuke(self, operation_id: str, delay: int):
        try:
            await asyncio.sleep(delay)
            await self._execute_nuke(operation_id)
        except asyncio.CancelledError:
            raise
        except Exception as exc:  # pragma: no cover
            logger.error("Nuke job failed: %s", exc, exc_info=True)
            async with self._nuke_lock:
                if self._pending_nuke and self._pending_nuke.get("id") == operation_id:
                    self._pending_nuke = None
                self._nuke_task = None

    async def _execute_nuke(self, operation_id: str):
        async with self._nuke_lock:
            if not self._pending_nuke or self._pending_nuke.get("id") != operation_id:
                return
            self._pending_nuke["status"] = "running"

        vector_deleted = 0
        storage_deleted = 0

        # ğŸ­ æ ¸çˆ†åŠŸèƒ½ä»…ä¸ºè§†è§‰æ•ˆæœï¼Œä¸ä¼šçœŸå®åˆ é™¤æ•°æ®
        # è¿™æ˜¯ä¸€ä¸ªå¨±ä¹æ€§çš„è§†è§‰ç‰¹æ•ˆï¼Œä¿æŠ¤ç”¨æˆ·æ•°æ®å®‰å…¨
        try:
            # åªç»Ÿè®¡æ•°é‡ç”¨äºæ˜¾ç¤ºï¼Œä¸æ‰§è¡Œä»»ä½•åˆ é™¤æ“ä½œ
            logger.info("æ ¸çˆ†è§†è§‰æ•ˆæœè§¦å‘ï¼šè¿™åªæ˜¯æ¨¡æ‹Ÿï¼Œä¸ä¼šåˆ é™¤ä»»ä½•æ•°æ®")

            # ç»Ÿè®¡è®°å¿†æ•°é‡ç”¨äºæ˜¾ç¤º
            async with self.faiss_manager.db.document_storage.connection.execute(
                "SELECT COUNT(*) FROM documents"
            ) as cursor:
                row = await cursor.fetchone()
                vector_deleted = row[0] if row else 0

            if self.memory_storage:
                async with self.memory_storage.connection.execute(
                    "SELECT COUNT(*) FROM memories"
                ) as cursor:
                    row = await cursor.fetchone()
                    storage_deleted = row[0] if row else 0

            logger.info(
                "æ ¸çˆ†è§†è§‰æ•ˆæœå®Œæˆï¼šæ¨¡æ‹Ÿæ¸…é™¤ %s æ¡å‘é‡è®°å½•å’Œ %s æ¡ç»“æ„åŒ–è®°å½•ï¼ˆå®é™…æ•°æ®å®Œå…¨æœªå—å½±å“ï¼‰",
                vector_deleted,
                storage_deleted,
            )
        except Exception as exc:
            logger.error("Memory wipe failed: %s", exc, exc_info=True)
        finally:
            async with self._nuke_lock:
                if self._pending_nuke and self._pending_nuke.get("id") == operation_id:
                    self._pending_nuke = None
                self._nuke_task = None

    def _auth_dependency(self):
        async def dependency(request: Request) -> str:
            token = self._extract_token(request)
            if not token:
                raise HTTPException(status.HTTP_401_UNAUTHORIZED, detail="æœªæˆæƒ")
            await self._validate_token(token)
            return token

        return dependency

    async def _validate_token(self, token: str):
        """
        éªŒè¯ token - ä¿®å¤: æ£€æŸ¥ç»å¯¹è¿‡æœŸæ—¶é—´å’Œä¼šè¯è¶…æ—¶
        """
        async with self._token_lock:
            await self._cleanup_tokens_locked()
            token_data = self._tokens.get(token)

            if not token_data:
                raise HTTPException(status.HTTP_401_UNAUTHORIZED, detail="ä¼šè¯å·²å¤±æ•ˆ")

            now = time.time()

            # æ£€æŸ¥ç»å¯¹è¿‡æœŸæ—¶é—´ (24å°æ—¶)
            if now - token_data["created_at"] > token_data["max_lifetime"]:
                self._tokens.pop(token, None)
                raise HTTPException(status.HTTP_401_UNAUTHORIZED, detail="ä¼šè¯å·²è¾¾æœ€å¤§æ—¶é•¿")

            # æ£€æŸ¥ä¼šè¯è¶…æ—¶ (æœ€åæ´»åŠ¨æ—¶é—´)
            if now - token_data["last_active"] > self.session_timeout:
                self._tokens.pop(token, None)
                raise HTTPException(status.HTTP_401_UNAUTHORIZED, detail="ä¼šè¯å·²è¿‡æœŸ")

            # æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´
            token_data["last_active"] = now

    async def _cleanup_tokens_locked(self):
        """
        æ¸…ç†è¿‡æœŸ token - ä¿®å¤: é€‚é…æ–°çš„ token æ•°æ®ç»“æ„
        """
        now = time.time()
        expired = []

        for token, token_data in self._tokens.items():
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ç»å¯¹è¿‡æœŸæ—¶é—´æˆ–ä¼šè¯è¶…æ—¶
            if (now - token_data["created_at"] > token_data["max_lifetime"] or
                now - token_data["last_active"] > self.session_timeout):
                expired.append(token)

        for token in expired:
            self._tokens.pop(token, None)

    def _extract_token(self, request: Request) -> str:
        auth_header = request.headers.get("Authorization", "")
        if auth_header.startswith("Bearer "):
            return auth_header[7:].strip()
        cookie_token = request.cookies.get("auth_token")
        if cookie_token:
            return cookie_token.strip()
        custom_header = request.headers.get("X-Auth-Token", "")
        return custom_header.strip()

    @staticmethod
    def _safe_json_loads(payload: str) -> Dict[str, Any]:
        if not payload:
            return {}
        try:
            return json.loads(payload)
        except json.JSONDecodeError:
            return {}

    @staticmethod
    def _format_timestamp(value: Any) -> Optional[str]:
        if not value:
            return None
        if isinstance(value, (int, float)):
            return datetime.fromtimestamp(value).isoformat(sep=" ", timespec="seconds")
        if isinstance(value, str):
            try:
                return datetime.fromisoformat(value.replace("Z", "+00:00")).isoformat(
                    sep=" ", timespec="seconds"
                )
            except ValueError:
                return value
        return str(value)
